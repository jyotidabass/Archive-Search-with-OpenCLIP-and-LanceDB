{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c94724aa",
      "metadata": {
        "id": "c94724aa"
      },
      "source": [
        "# Archive Search with OpenCLIP and LanceDB\n",
        "\n",
        "In this example we'll build a Arxiv Search or a recommender based on semantic search using LanceDB. We'll also compare the results with keyword based saerch on Nomic's atlast\n",
        "\n",
        "\n",
        "## OpenCLIP\n",
        "\n",
        "![CLIP (1)](https://github.com/lancedb/vectordb-recipes/assets/15766192/11b3b900-0bcb-4a4a-8fd4-804611c85972)\n",
        "\n",
        "\n",
        "OpenCLIP an open source implementation of OpenAI's CLIP (Contrastive Language-Image Pre-training) as is available with various backends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d29cbe5",
      "metadata": {
        "id": "7d29cbe5",
        "outputId": "31d13464-e377-4906-ad91-1e6f022bed90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 6.2.0 requires pyarrow<13,>=2, but you have pyarrow 14.0.1 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# SETUP\n",
        "!pip install lancedb open_clip_torch arxiv --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "82cf5c34-7f41-4860-844c-c5aa2cd578de",
      "metadata": {
        "id": "82cf5c34-7f41-4860-844c-c5aa2cd578de",
        "outputId": "01a03ea6-1682-40f6-e315-ce849362980b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78088422",
      "metadata": {
        "id": "78088422"
      },
      "source": [
        "## Creating table from arxiv API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88cba25e",
      "metadata": {
        "id": "88cba25e"
      },
      "source": [
        "### Embedding Paper Summary using CLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fba615af",
      "metadata": {
        "id": "fba615af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import open_clip\n",
        "import pandas as pd\n",
        "from open_clip import tokenizer\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import arxiv\n",
        "import lancedb\n",
        "\n",
        "def embed_func_clip(text):\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(tokenizer(text))\n",
        "    return text_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5802574",
      "metadata": {
        "id": "e5802574"
      },
      "source": [
        "### Create a DataFrame of the desired length\n",
        "\n",
        "Here we'll use arxiv python utility to interact with arxiv api and get the document data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eb8afe10",
      "metadata": {
        "id": "eb8afe10"
      },
      "outputs": [],
      "source": [
        "def get_arxiv_df(embed_func, length=100):\n",
        "    results = arxiv.Search(\n",
        "      query= \"cat:cs.AI OR cat:cs.CV OR cat:stat.ML\",\n",
        "      max_results = length,\n",
        "      sort_by = arxiv.SortCriterion.Relevance,\n",
        "      sort_order = arxiv.SortOrder.Descending\n",
        "    ).results()\n",
        "    df = defaultdict(list)\n",
        "    for result in tqdm(results, total=length):\n",
        "        try:\n",
        "            df[\"title\"].append(result.title)\n",
        "            df[\"summary\"].append(result.summary)\n",
        "            df[\"authors\"].append(str(result.authors))\n",
        "            df[\"url\"].append(result.entry_id)\n",
        "            df[\"vector\"].append(embed_func(result.summary).tolist()[0])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"error: \", e)\n",
        "\n",
        "    return pd.DataFrame(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aa2edccf",
      "metadata": {
        "id": "aa2edccf"
      },
      "outputs": [],
      "source": [
        "LENGTH = 100 # Reduce the size for demo\n",
        "def create_table():\n",
        "    db = lancedb.connect(\"db\")\n",
        "    df = get_arxiv_df(embed_func_clip, LENGTH)\n",
        "\n",
        "    tbl = db.create_table(\"arxiv\", data=df, mode=\"overwrite\")\n",
        "\n",
        "    return tbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "760efa67-8742-4087-91c8-49465b4843b0",
      "metadata": {
        "id": "760efa67-8742-4087-91c8-49465b4843b0",
        "outputId": "cf8212a8-e419-4c20-e96a-ed454a6c394f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "d00e0e0fff364164988e7c418fc82ba3",
            "9981e71a2ec1487f8048a7295cbc848f",
            "85ce11f215154dcb99c37b9d25e41a76",
            "b8229f413e4f49c0bdde8d1796d99f0e",
            "389eed913b4d4333a9d8247980c3856e",
            "3ea64383f04d4082b7d2040406fb54c9",
            "c6dcdc475fc24fd7a4c622fcb3014827",
            "f112540eda6040cebb5ed60cd2a9aa98",
            "99ebec05a8724828be1d0e4399cada60",
            "5749d6501d72432e88bcba5a8d21ad24",
            "cdec8ae0ec5a44a9beed434d76affa29"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-32209edb23b5>:7: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
            "  ).results()\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d00e0e0fff364164988e7c418fc82ba3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [05:30<00:00,  3.30s/it]\n"
          ]
        }
      ],
      "source": [
        "tbl = create_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e9c04cb6",
      "metadata": {
        "id": "e9c04cb6"
      },
      "outputs": [],
      "source": [
        "import lancedb\n",
        "\n",
        "db = lancedb.connect(\"db\")\n",
        "\n",
        "if \"arxiv\" not in db.table_names():\n",
        "    tbl = create_table()\n",
        "else:\n",
        "    tbl = db.open_table(\"arxiv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d51fadf3-3f98-44fa-9ecd-0dbdfb2f9eb7",
      "metadata": {
        "id": "d51fadf3-3f98-44fa-9ecd-0dbdfb2f9eb7",
        "outputId": "f03b5c1c-6857-4557-c791-b15163b46672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(tbl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09adb9d3",
      "metadata": {
        "id": "09adb9d3"
      },
      "source": [
        "## Semantic Search by concepts or summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "acc38daa",
      "metadata": {
        "id": "acc38daa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def search_table(query, embed_func=embed_func_clip, lim=3):\n",
        "    db = lancedb.connect(\"db\")\n",
        "    tbl = db.open_table(\"arxiv\")\n",
        "\n",
        "    embs = embed_func(query)\n",
        "\n",
        "    return tbl.search(embs.tolist()[0]).limit(3).to_df()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27391ad7-433e-4b32-8215-534d47de08d8",
      "metadata": {
        "id": "27391ad7-433e-4b32-8215-534d47de08d8",
        "outputId": "918a0797-a83f-480b-d3b7-e2b408ea2901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(tbl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "971be6ef",
      "metadata": {
        "id": "971be6ef",
        "outputId": "341cc5b4-9d01-428d-ab4e-46094b624a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d07584fcae98>:9: DeprecatedWarning: to_df is deprecated as of 0.3.1 and will be removed in 0.4.0. Use to_pandas() instead\n",
            "  return tbl.search(embs.tolist()[0]).limit(3).to_df()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>authors</th>\n",
              "      <th>url</th>\n",
              "      <th>_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average</td>\n",
              "      <td>Presently the most successful approaches to semi-supervised learning are\\nbased on consistency regularization, whereby a model is trained to be robust to\\nsmall perturbations of its inputs and parameters. To understand consistency\\nregularization, we conceptually explore how loss geometry interacts with\\ntraining procedures. The consistency loss dramatically improves generalization\\nperformance over supervised-only training; however, we show that SGD struggles\\nto converge on the consistency loss and continues to make large steps that lead\\nto changes in predictions on the test data. Motivated by these observations, we\\npropose to train consistency-based methods with Stochastic Weight Averaging\\n(SWA), a recent approach which averages weights along the trajectory of SGD\\nwith a modified learning rate schedule. We also propose fast-SWA, which further\\naccelerates convergence by averaging multiple points within each cycle of a\\ncyclical learning rate schedule. With weight averaging, we achieve the best\\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\\nquantities of labeled training data. For example, we achieve 5.0% error on\\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\\nliterature of 6.3%.</td>\n",
              "      <td>[arxiv.Result.Author('Ben Athiwaratkun'), arxiv.Result.Author('Marc Finzi'), arxiv.Result.Author('Pavel Izmailov'), arxiv.Result.Author('Andrew Gordon Wilson')]</td>\n",
              "      <td>http://arxiv.org/abs/1806.05594v3</td>\n",
              "      <td>39.569557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification</td>\n",
              "      <td>In recent years, there have been numerous developments towards solving\\nmultimodal tasks, aiming to learn a stronger representation than through a\\nsingle modality. Certain aspects of the data can be particularly useful in this\\ncase - for example, correlations in the space or time domain across modalities\\n- but should be wisely exploited in order to benefit from their full predictive\\npotential. We propose two deep learning architectures with multimodal\\ncross-connections that allow for dataflow between several feature extractors\\n(XFlow). Our models derive more interpretable features and achieve better\\nperformances than models which do not exchange representations, usefully\\nexploiting correlations between audio and visual data, which have a different\\ndimensionality and are nontrivially exchangeable. Our work improves on existing\\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\\nnovel method for performing cross-modality (before features are learned from\\nindividual modalities) and (2) extends the previously proposed\\ncross-connections which only transfer information between streams that process\\ncompatible data. Illustrating some of the representations learned by the\\nconnections, we analyse their contribution to the increase in discrimination\\nability and reveal their compatibility with a lip-reading network intermediate\\nrepresentation. We provide the research community with Digits, a new dataset\\nconsisting of three data types extracted from videos of people saying the\\ndigits 0-9. Results show that both cross-modal architectures outperform their\\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\\ndatasets, achieving state-of-the-art results.</td>\n",
              "      <td>[arxiv.Result.Author('Cătălina Cangea'), arxiv.Result.Author('Petar Veličković'), arxiv.Result.Author('Pietro Liò')]</td>\n",
              "      <td>http://arxiv.org/abs/1709.00572v2</td>\n",
              "      <td>40.346905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Hybrid Instance-based Transfer Learning Method</td>\n",
              "      <td>In recent years, supervised machine learning models have demonstrated\\ntremendous success in a variety of application domains. Despite the promising\\nresults, these successful models are data hungry and their performance relies\\nheavily on the size of training data. However, in many healthcare applications\\nit is difficult to collect sufficiently large training datasets. Transfer\\nlearning can help overcome this issue by transferring the knowledge from\\nreadily available datasets (source) to a new dataset (target). In this work, we\\npropose a hybrid instance-based transfer learning method that outperforms a set\\nof baselines including state-of-the-art instance-based transfer learning\\napproaches. Our method uses a probabilistic weighting strategy to fuse\\ninformation from the source domain to the model learned in the target domain.\\nOur method is generic, applicable to multiple source domains, and robust with\\nrespect to negative transfer. We demonstrate the effectiveness of our approach\\nthrough extensive experiments for two different applications.</td>\n",
              "      <td>[arxiv.Result.Author('Azin Asgarian'), arxiv.Result.Author('Parinaz Sobhani'), arxiv.Result.Author('Ji Chao Zhang'), arxiv.Result.Author('Madalin Mihailescu'), arxiv.Result.Author('Ariel Sibilia'), arxiv.Result.Author('Ahmed Bilal Ashraf'), arxiv.Result.Author('Babak Taati')]</td>\n",
              "      <td>http://arxiv.org/abs/1812.01063v1</td>\n",
              "      <td>41.702740</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# MobileSAM paper abstract 2nd half\n",
        "query = \"\"\"\n",
        "Many of such applications need to be run on resource-constraint edge devices,\n",
        "like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\n",
        "image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\n",
        "paper leads to unsatisfactory performance, especially when limited training sources are available. We\n",
        "find that this is mainly caused by the coupled optimization of the image encoder and mask decoder,\n",
        "motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from\n",
        "the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be\n",
        "automatically compatible with the mask decoder in the original SAM. The training can be completed\n",
        "on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM\n",
        "which is more than 60 times smaller yet performs on par with the original SAM. For inference speed,\n",
        "With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms\n",
        "on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the\n",
        "concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover,\n",
        "we show that MobileSAM can run relatively smoothly on CPU\n",
        "\"\"\"\n",
        "\n",
        "result = search_table(query)\n",
        "\n",
        "result.pop(\"vector\")\n",
        "display(HTML(result.to_html()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4ccd273",
      "metadata": {
        "id": "f4ccd273",
        "outputId": "c56f0fa8-b02a-47e9-e300-5e4119bacdd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d07584fcae98>:9: DeprecatedWarning: to_df is deprecated as of 0.3.1 and will be removed in 0.4.0. Use to_pandas() instead\n",
            "  return tbl.search(embs.tolist()[0]).limit(3).to_df()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>authors</th>\n",
              "      <th>url</th>\n",
              "      <th>_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Is 'Unsupervised Learning' a Misconceived Term?</td>\n",
              "      <td>Is all of machine learning supervised to some degree? The field of machine\\nlearning has traditionally been categorized pedagogically into\\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\\nreferred to learning from labeled data, while unsupervised learning has\\ntypically referred to learning from unlabeled data. In this paper, we assert\\nthat all machine learning is in fact supervised to some degree, and that the\\nscope of supervision is necessarily commensurate to the scope of learning\\npotential. In particular, we argue that clustering algorithms such as k-means,\\nand dimensionality reduction algorithms such as principal component analysis,\\nvariational autoencoders, and deep belief networks are each internally\\nsupervised by the data itself to learn their respective representations of its\\nfeatures. Furthermore, these algorithms are not capable of external inference\\nuntil their respective outputs (clusters, principal components, or\\nrepresentation codes) have been identified and externally labeled in effect. As\\nsuch, they do not suffice as examples of unsupervised learning. We propose that\\nthe categorization `supervised vs unsupervised learning' be dispensed with, and\\ninstead, learning algorithms be categorized as either\\n$internally~or~externally~supervised$ (or both). We believe this change in\\nperspective will yield new fundamental insights into the structure and\\ncharacter of data and of learning algorithms.</td>\n",
              "      <td>[arxiv.Result.Author('Stephen G. Odaibo')]</td>\n",
              "      <td>http://arxiv.org/abs/1904.03259v1</td>\n",
              "      <td>30.549643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quantizing Convolutional Neural Networks for Low-Power High-Throughput Inference Engines</td>\n",
              "      <td>Deep learning as a means to inferencing has proliferated thanks to its\\nversatility and ability to approach or exceed human-level accuracy. These\\ncomputational models have seemingly insatiable appetites for computational\\nresources not only while training, but also when deployed at scales ranging\\nfrom data centers all the way down to embedded devices. As such, increasing\\nconsideration is being made to maximize the computational efficiency given\\nlimited hardware and energy resources and, as a result, inferencing with\\nreduced precision has emerged as a viable alternative to the IEEE 754 Standard\\nfor Floating-Point Arithmetic. We propose a quantization scheme that allows\\ninferencing to be carried out using arithmetic that is fundamentally more\\nefficient when compared to even half-precision floating-point. Our quantization\\nprocedure is significant in that we determine our quantization scheme\\nparameters by calibrating against its reference floating-point model using a\\nsingle inference batch rather than (re)training and achieve end-to-end post\\nquantization accuracies comparable to the reference model.</td>\n",
              "      <td>[arxiv.Result.Author('Sean O. Settle'), arxiv.Result.Author('Manasa Bollavaram'), arxiv.Result.Author(\"Paolo D'Alberto\"), arxiv.Result.Author('Elliott Delaye'), arxiv.Result.Author('Oscar Fernandez'), arxiv.Result.Author('Nicholas Fraser'), arxiv.Result.Author('Aaron Ng'), arxiv.Result.Author('Ashish Sirasao'), arxiv.Result.Author('Michael Wu')]</td>\n",
              "      <td>http://arxiv.org/abs/1805.07941v1</td>\n",
              "      <td>33.038021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Task-Free Continual Learning</td>\n",
              "      <td>Methods proposed in the literature towards continual deep learning typically\\noperate in a task-based sequential learning setup. A sequence of tasks is\\nlearned, one at a time, with all data of current task available but not of\\nprevious or future tasks. Task boundaries and identities are known at all\\ntimes. This setup, however, is rarely encountered in practical applications.\\nTherefore we investigate how to transform continual learning to an online\\nsetup. We develop a system that keeps on learning over time in a streaming\\nfashion, with data distributions gradually changing and without the notion of\\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\\nshow how this method can be made online by providing a protocol to decide i)\\nwhen to update the importance weights, ii) which data to use to update them,\\nand iii) how to accumulate the importance weights at each update step.\\nExperimental results show the validity of the approach in the context of two\\napplications: (self-)supervised learning of a face recognition model by\\nwatching soap series and learning a robot to avoid collisions.</td>\n",
              "      <td>[arxiv.Result.Author('Rahaf Aljundi'), arxiv.Result.Author('Klaas Kelchtermans'), arxiv.Result.Author('Tinne Tuytelaars')]</td>\n",
              "      <td>http://arxiv.org/abs/1812.03596v3</td>\n",
              "      <td>35.886036</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Exmaple 2: Search via a concept you're reading\n",
        "query = \"\"\"\n",
        "What is the general idea behind self-supervised learning.\n",
        "\"\"\"\n",
        "\n",
        "result = search_table(query)\n",
        "\n",
        "result.pop(\"vector\")\n",
        "display(HTML(result.to_html()))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "511a7c77cb034b09af5465c01316a0f4bb20176d139e60e6d7915f9a637a5037"
      }
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d00e0e0fff364164988e7c418fc82ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9981e71a2ec1487f8048a7295cbc848f",
              "IPY_MODEL_85ce11f215154dcb99c37b9d25e41a76",
              "IPY_MODEL_b8229f413e4f49c0bdde8d1796d99f0e"
            ],
            "layout": "IPY_MODEL_389eed913b4d4333a9d8247980c3856e"
          }
        },
        "9981e71a2ec1487f8048a7295cbc848f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea64383f04d4082b7d2040406fb54c9",
            "placeholder": "​",
            "style": "IPY_MODEL_c6dcdc475fc24fd7a4c622fcb3014827",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "85ce11f215154dcb99c37b9d25e41a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f112540eda6040cebb5ed60cd2a9aa98",
            "max": 605219813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99ebec05a8724828be1d0e4399cada60",
            "value": 605219813
          }
        },
        "b8229f413e4f49c0bdde8d1796d99f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5749d6501d72432e88bcba5a8d21ad24",
            "placeholder": "​",
            "style": "IPY_MODEL_cdec8ae0ec5a44a9beed434d76affa29",
            "value": " 605M/605M [00:09&lt;00:00, 74.2MB/s]"
          }
        },
        "389eed913b4d4333a9d8247980c3856e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea64383f04d4082b7d2040406fb54c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6dcdc475fc24fd7a4c622fcb3014827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f112540eda6040cebb5ed60cd2a9aa98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ebec05a8724828be1d0e4399cada60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5749d6501d72432e88bcba5a8d21ad24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdec8ae0ec5a44a9beed434d76affa29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}